"""
Paper Pipeline ä¸»å…¥å£
æ”¯æŒ: çˆ¬å– -> ç­›é€‰ -> HTML ç”Ÿæˆ (æ¯ä¸ªæ­¥éª¤å¯å•ç‹¬è·³è¿‡)
"""
import argparse
import os
import pandas as pd
from typing import List, Optional

from config import Config, load_config
from crawlers import OpenReviewCrawler, ArxivCrawler, PaperData
from filters import FineFilter
from prompt_generator import generate_all
from output import write_csv, write_html
from output.html_writer import write_html_from_csv


def interactive_confirm_keywords(keywords: List[str]) -> List[str]:
    """äº¤äº’å¼ç¡®è®¤å…³é”®è¯"""
    print("\n" + "="*60)
    print("ğŸ”‘ è¯·ç¡®è®¤æœç´¢å…³é”®è¯")
    print("="*60)
    print("LLM ç”Ÿæˆçš„å…³é”®è¯:")
    for i, kw in enumerate(keywords, 1):
        print(f"  {i}. {kw}")
    
    print("\né€‰é¡¹:")
    print("  [Enter] ä½¿ç”¨å½“å‰å…³é”®è¯ç»§ç»­")
    print("  [e] ç¼–è¾‘å…³é”®è¯ï¼ˆè¾“å…¥æ–°çš„å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼‰")
    print("  [q] é€€å‡º")
    
    choice = input("\nè¯·é€‰æ‹© [Enter/e/q]: ").strip().lower()
    
    if choice == 'q':
        print("ç”¨æˆ·å–æ¶ˆ")
        exit(0)
    elif choice == 'e':
        new_keywords = input("è¯·è¾“å…¥æ–°çš„å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰: ").strip()
        if new_keywords:
            keywords = [kw.strip() for kw in new_keywords.split(",") if kw.strip()]
            print(f"âœ… æ›´æ–°å…³é”®è¯: {keywords}")
    
    return keywords


def interactive_confirm_prompt(prompt_type: str, prompt: str) -> str:
    """äº¤äº’å¼ç¡®è®¤ Prompt"""
    print("\n" + "="*60)
    print(f"ğŸ“ è¯·ç¡®è®¤{prompt_type} Prompt")
    print("="*60)
    print(prompt[:600] + "..." if len(prompt) > 600 else prompt)
    
    print("\né€‰é¡¹:")
    print("  [Enter] ä½¿ç”¨å½“å‰ Prompt ç»§ç»­")
    print("  [e] ç¼–è¾‘ Prompt")
    print("  [q] é€€å‡º")
    
    choice = input("\nè¯·é€‰æ‹© [Enter/e/q]: ").strip().lower()
    
    if choice == 'q':
        print("ç”¨æˆ·å–æ¶ˆ")
        exit(0)
    elif choice == 'e':
        print("è¯·è¾“å…¥æ–°çš„ Promptï¼ˆè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:")
        lines = []
        while True:
            line = input()
            if line == "":
                break
            lines.append(line)
        if lines:
            prompt = "\n".join(lines)
            print(f"âœ… Prompt å·²æ›´æ–°")
    
    return prompt


def interactive_confirm_crawl_result(paper_count: int) -> bool:
    """ç¡®è®¤æ˜¯å¦ç»§ç»­è¿›è¡Œ LLM ç­›é€‰"""
    print("\n" + "="*60)
    print(f"ğŸ“Š å…±çˆ¬å– {paper_count} ç¯‡è®ºæ–‡")
    print("="*60)
    print("é€‰é¡¹:")
    print("  [Enter] ç»§ç»­è¿›è¡Œ LLM ç­›é€‰")
    print("  [q] é€€å‡ºï¼ˆä¿ç•™å·²çˆ¬å–çš„æ•°æ®ï¼‰")
    
    choice = input("\nè¯·é€‰æ‹© [Enter/q]: ").strip().lower()
    return choice != 'q'


def deduplicate_papers(papers: List[PaperData]) -> List[PaperData]:
    """æŒ‰æ ‡é¢˜å»é‡"""
    seen_titles = set()
    unique_papers = []
    
    for paper in papers:
        normalized_title = ' '.join(paper.title.lower().strip().split())
        if normalized_title not in seen_titles:
            seen_titles.add(normalized_title)
            unique_papers.append(paper)
    
    return unique_papers


def crawl_papers(config: Config, keywords: List[str]) -> List[PaperData]:
    """çˆ¬å–è®ºæ–‡"""
    all_papers = []
    
    print("\n" + "="*60)
    print("ğŸ“š æ­¥éª¤ 2/4: çˆ¬å–è®ºæ–‡")
    print("="*60)
    print(f"ğŸ”‘ ä½¿ç”¨å…³é”®è¯: {', '.join(keywords)}")
    
    # çˆ¬å– OpenReview ä¼šè®®
    for year in config.years:
        crawler = OpenReviewCrawler(year)
        for conf in config.conferences:
            print(f"\n--- {conf} {year} ---")
            papers = crawler.crawl(conf, keywords=keywords)
            all_papers.extend(papers)
    
    openreview_count = len(all_papers)
    
    # çˆ¬å– Arxiv
    if config.crawl_arxiv and keywords:
        print(f"\n--- Arxiv ---")
        arxiv_crawler = ArxivCrawler(min_citations=5)
        arxiv_years = list(range(min(config.years) - 1, max(config.years) + 1))
        papers = arxiv_crawler.crawl(
            keywords, 
            years=arxiv_years,
            max_results=config.arxiv_max_results,
            filter_by_keywords=True,
            filter_by_citations=True
        )
        all_papers.extend(papers)
    
    # å»é‡
    print(f"\nğŸ”„ å»é‡ä¸­...")
    before_dedup = len(all_papers)
    all_papers = deduplicate_papers(all_papers)
    
    if before_dedup > len(all_papers):
        print(f"   ç§»é™¤ {before_dedup - len(all_papers)} ç¯‡é‡å¤è®ºæ–‡")
    
    print(f"\nğŸ“Š æ€»è®¡: {len(all_papers)} ç¯‡è®ºæ–‡")
    
    # ä¿å­˜åŸå§‹æ•°æ®
    write_csv(all_papers, config.raw_papers_path, include_filter_results=False)
    
    return all_papers


def run_pipeline(
    user_description: str,
    config: Optional[Config] = None,
    skip_crawl: bool = False,
    skip_filter: bool = False,
    html_only: bool = False,
    interactive: bool = True,
):
    """
    è¿è¡Œè®ºæ–‡ç­›é€‰ Pipeline
    
    Args:
        user_description: ç”¨æˆ·çš„ç ”ç©¶éœ€æ±‚æè¿°
        config: Pipeline é…ç½®
        skip_crawl: è·³è¿‡çˆ¬å–æ­¥éª¤
        skip_filter: è·³è¿‡ç­›é€‰æ­¥éª¤ï¼ˆç›´æ¥ä» raw æˆ– filtered CSV ç”Ÿæˆ HTMLï¼‰
        html_only: åªç”Ÿæˆ HTMLï¼ˆä» papers_filtered.csvï¼‰
        interactive: æ˜¯å¦å¯ç”¨äº¤äº’æ¨¡å¼
    """
    if config is None:
        config = load_config()
    
    # åªç”Ÿæˆ HTML
    if html_only:
        print("\n" + "="*60)
        print("ğŸ“„ åªç”Ÿæˆ HTML æŠ¥å‘Š")
        print("="*60)
        write_html_from_csv(
            config.output_csv_path, 
            config.output_html_path,
            subtitle=f"åŸºäº: {user_description[:50]}..."
        )
        return
    
    print("\n" + "="*60)
    print("ğŸš€ Paper Pipeline å¯åŠ¨")
    print("="*60)
    print(f"ğŸ“… å¹´ä»½: {config.years}")
    print(f"ğŸ“ ä¼šè®®: {config.conferences}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"ğŸ”„ äº¤äº’æ¨¡å¼: {'å¼€å¯' if interactive else 'å…³é—­'}")
    
    # Step 1: ç”Ÿæˆå…³é”®è¯å’Œ Promptï¼ˆå¦‚æœéœ€è¦ï¼‰
    keywords = []
    filter_prompt = ""
    
    if not skip_crawl or not skip_filter:
        print("\n" + "="*60)
        print("ğŸ§  æ­¥éª¤ 1/4: ç”Ÿæˆå…³é”®è¯å’Œç­›é€‰ Prompt")
        print("="*60)
        
        generated = generate_all(user_description, config.large_llm)
        keywords = generated["keywords"]
        filter_prompt = generated["fine_prompt"]
        
        if not keywords and not skip_crawl:
            print("âŒ æœªç”Ÿæˆå…³é”®è¯ï¼Œæµç¨‹ç»ˆæ­¢ã€‚è¯·æ£€æŸ¥ API é…ç½®ã€‚")
            return
        
        # äº¤äº’ç¡®è®¤å…³é”®è¯
        if interactive and not skip_crawl:
            keywords = interactive_confirm_keywords(keywords)
    
    # Step 2: çˆ¬å–è®ºæ–‡
    if skip_crawl and os.path.exists(config.raw_papers_path):
        print("\nâ­ï¸ è·³è¿‡çˆ¬å–ï¼ŒåŠ è½½å·²æœ‰æ•°æ®...")
        df = pd.read_csv(config.raw_papers_path)
        papers = [PaperData.from_dict(row) for _, row in df.iterrows()]
        print(f"   åŠ è½½äº† {len(papers)} ç¯‡è®ºæ–‡")
    else:
        papers = crawl_papers(config, keywords)
    
    if not papers:
        print("âŒ æœªè·å–åˆ°è®ºæ–‡ï¼Œæµç¨‹ç»ˆæ­¢")
        return
    
    # äº¤äº’ç¡®è®¤æ˜¯å¦ç»§ç»­
    if interactive:
        if not interactive_confirm_crawl_result(len(papers)):
            print("âœ… æ•°æ®å·²ä¿å­˜ï¼Œæµç¨‹ç»ˆæ­¢")
            return
    
    # Step 3: LLM ç­›é€‰
    if skip_filter:
        print("\nâ­ï¸ è·³è¿‡ç­›é€‰ï¼Œç›´æ¥ç”Ÿæˆ HTML...")
        filtered_papers = papers
    else:
        print("\n" + "="*60)
        print("ğŸ¯ æ­¥éª¤ 3/4: DeepSeek ç­›é€‰")
        print("="*60)
        
        if interactive:
            filter_prompt = interactive_confirm_prompt("ç­›é€‰", filter_prompt)
        
        llm_filter = FineFilter(config.large_llm, filter_prompt, concurrency=config.concurrency)
        filtered_papers = llm_filter.filter_papers(papers)
        
        if not filtered_papers:
            print("âš ï¸ ç­›é€‰åæ— è®ºæ–‡é€šè¿‡")
            return
        
        # ä¿å­˜ç­›é€‰ç»“æœ
        write_csv(filtered_papers, config.output_csv_path, include_filter_results=True)
    
    # Step 4: è¾“å‡º HTML
    print("\n" + "="*60)
    print("ğŸ’¾ æ­¥éª¤ 4/4: è¾“å‡ºç»“æœ")
    print("="*60)
    
    write_html(
        filtered_papers, 
        config.output_html_path,
        subtitle=f"åŸºäº: {user_description[:50]}..."
    )
    
    # å®Œæˆ
    print("\n" + "="*60)
    print("ğŸ‰ Pipeline å®Œæˆ!")
    print("="*60)
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"   - çˆ¬å–è®ºæ–‡: {len(papers)} ç¯‡")
    print(f"   - ç­›é€‰é€šè¿‡: {len(filtered_papers)} ç¯‡")
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   - CSV: {os.path.abspath(config.output_csv_path)}")
    print(f"   - HTML: {os.path.abspath(config.output_html_path)}")


def main():
    parser = argparse.ArgumentParser(
        description="è®ºæ–‡çˆ¬å–ä¸ç­›é€‰ Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python main.py -d "LLM ç½®ä¿¡åº¦ä¼°è®¡çš„ç ”ç©¶"
  python main.py -d "..." --skip-crawl          # è·³è¿‡çˆ¬å–
  python main.py -d "..." --skip-filter         # è·³è¿‡ç­›é€‰
  python main.py -d "..." --html-only           # åªç”Ÿæˆ HTML
  python main.py -d "..." --no-interactive      # éäº¤äº’æ¨¡å¼
        """
    )
    
    parser.add_argument(
        "--description", "-d",
        type=str,
        required=True,
        help="ä½ çš„ç ”ç©¶éœ€æ±‚æè¿°"
    )
    
    parser.add_argument(
        "--years", "-y",
        type=str,
        default="2024,2025",
        help="è¦çˆ¬å–çš„å¹´ä»½ (é»˜è®¤: 2024,2025)"
    )
    
    parser.add_argument(
        "--conferences", "-c",
        type=str,
        default="ICLR,ICML,NEURIPS,ACL",
        help="è¦çˆ¬å–çš„ä¼šè®® (é»˜è®¤: ICLR,ICML,NEURIPS,ACL)"
    )
    
    parser.add_argument(
        "--no-arxiv",
        action="store_true",
        help="ä¸çˆ¬å– Arxiv"
    )
    
    parser.add_argument(
        "--no-interactive",
        action="store_true",
        help="å…³é—­äº¤äº’æ¨¡å¼"
    )
    
    parser.add_argument(
        "--output-dir", "-o",
        type=str,
        default="./output",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: ./output)"
    )
    
    parser.add_argument(
        "--skip-crawl",
        action="store_true",
        help="è·³è¿‡çˆ¬å–æ­¥éª¤ï¼Œä½¿ç”¨å·²æœ‰çš„åŸå§‹æ•°æ®"
    )
    
    parser.add_argument(
        "--skip-filter",
        action="store_true",
        help="è·³è¿‡ LLM ç­›é€‰æ­¥éª¤"
    )
    
    parser.add_argument(
        "--html-only",
        action="store_true",
        help="åªä» CSV ç”Ÿæˆ HTMLï¼ˆè·³è¿‡çˆ¬å–å’Œç­›é€‰ï¼‰"
    )
    
    args = parser.parse_args()
    
    # è§£æå‚æ•°
    years = [int(y.strip()) for y in args.years.split(",")]
    conferences = [c.strip().upper() for c in args.conferences.split(",")]
    
    # åˆ›å»ºé…ç½®
    config = load_config(
        years=years,
        conferences=conferences,
        crawl_arxiv=not args.no_arxiv,
        output_dir=args.output_dir
    )
    
    # è¿è¡Œ Pipeline
    run_pipeline(
        user_description=args.description,
        config=config,
        skip_crawl=args.skip_crawl,
        skip_filter=args.skip_filter,
        html_only=args.html_only,
        interactive=not args.no_interactive
    )


if __name__ == "__main__":
    main()
