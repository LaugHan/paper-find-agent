"""
Arxiv çˆ¬è™« - æ”¯æŒå…³é”®è¯è¿‡æ»¤ã€å¹´ä»½è¿‡æ»¤å’Œå¼•ç”¨é‡è¿‡æ»¤
"""
import arxiv
import requests
import time
from typing import List, Optional, Dict
from .base import PaperData


def matches_keywords(title: str, abstract: str, keywords: List[str]) -> bool:
    """æ£€æŸ¥è®ºæ–‡æ˜¯å¦åŒ¹é…å…³é”®è¯"""
    if not keywords:
        return True
    full_text = f"{title} {abstract}".lower()
    for kw in keywords:
        if kw.lower() in full_text:
            return True
    return False


def get_arxiv_id_from_url(url: str) -> Optional[str]:
    """ä» arxiv URL æå– arxiv ID"""
    # URL æ ¼å¼: http://arxiv.org/abs/2301.12345v1
    if 'arxiv.org' in url:
        parts = url.split('/')
        for i, part in enumerate(parts):
            if part == 'abs' and i + 1 < len(parts):
                arxiv_id = parts[i + 1]
                # ç§»é™¤ç‰ˆæœ¬å·
                if 'v' in arxiv_id:
                    arxiv_id = arxiv_id.split('v')[0]
                return arxiv_id
    return None


def get_citation_count_batch(arxiv_ids: List[str]) -> Dict[str, int]:
    """
    æ‰¹é‡è·å– arxiv è®ºæ–‡çš„å¼•ç”¨é‡ï¼ˆä½¿ç”¨ Semantic Scholar APIï¼‰
    
    Args:
        arxiv_ids: arxiv ID åˆ—è¡¨
    
    Returns:
        {arxiv_id: citation_count} å­—å…¸
    """
    if not arxiv_ids:
        return {}
    
    results = {}
    
    # Semantic Scholar API æ”¯æŒæ‰¹é‡æŸ¥è¯¢ï¼ˆæ¯æ¬¡æœ€å¤š 500 ä¸ªï¼‰
    # ä½¿ç”¨ paper/batch ç«¯ç‚¹
    batch_size = 100
    
    for i in range(0, len(arxiv_ids), batch_size):
        batch = arxiv_ids[i:i + batch_size]
        
        # æ„å»ºè¯·æ±‚
        ids = [f"ARXIV:{aid}" for aid in batch]
        
        try:
            response = requests.post(
                "https://api.semanticscholar.org/graph/v1/paper/batch",
                json={"ids": ids},
                params={"fields": "citationCount,externalIds"},
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                for paper in data:
                    if paper and paper.get('externalIds', {}).get('ArXiv'):
                        arxiv_id = paper['externalIds']['ArXiv']
                        results[arxiv_id] = paper.get('citationCount', 0) or 0
            elif response.status_code == 429:
                print("   âš ï¸ Semantic Scholar API é™æµï¼Œç­‰å¾… 5 ç§’...")
                time.sleep(5)
            else:
                print(f"   âš ï¸ Semantic Scholar API é”™è¯¯: {response.status_code}")
                
        except Exception as e:
            print(f"   âš ï¸ è·å–å¼•ç”¨é‡å¤±è´¥: {e}")
        
        # é™æµä¿æŠ¤
        time.sleep(1)
    
    return results


class ArxivCrawler:
    """Arxiv è®ºæ–‡çˆ¬è™«"""
    
    def __init__(self, min_citations: int = 5):
        """
        Args:
            min_citations: æœ€å°å¼•ç”¨é‡ï¼ˆä½äºæ­¤å€¼çš„è®ºæ–‡å°†è¢«è¿‡æ»¤ï¼‰
        """
        self.client = arxiv.Client(
            page_size=100,
            delay_seconds=3.0,
            num_retries=3
        )
        self.min_citations = min_citations
    
    def crawl(
        self,
        keywords: List[str],
        years: Optional[List[int]] = None,
        max_results: int = 500,
        categories: Optional[List[str]] = None,
        filter_by_keywords: bool = True,
        filter_by_citations: bool = True
    ) -> List[PaperData]:
        """
        çˆ¬å– Arxiv è®ºæ–‡
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            years: å¹´ä»½åˆ—è¡¨ï¼Œåªä¿ç•™è¿™äº›å¹´ä»½çš„è®ºæ–‡
            max_results: æœ€å¤§ç»“æœæ•°
            categories: Arxiv åˆ†ç±» (é»˜è®¤: cs.CL, cs.LG, cs.AI)
            filter_by_keywords: æ˜¯å¦å¯¹ç»“æœè¿›è¡Œå…³é”®è¯è¿‡æ»¤
            filter_by_citations: æ˜¯å¦æŒ‰å¼•ç”¨é‡è¿‡æ»¤
        
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        if not keywords:
            return []
        
        if categories is None:
            categories = ['cs.CL', 'cs.LG', 'cs.AI']
        
        # æ„å»ºæŸ¥è¯¢
        keywords_query = " OR ".join([f'abs:"{k}"' for k in keywords])
        category_query = " OR ".join([f'cat:{c}' for c in categories])
        final_query = f'({keywords_query}) AND ({category_query})'
        
        print(f"ğŸ” [Arxiv] æœç´¢: {final_query[:100]}...")
        if years:
            print(f"   ğŸ“… å¹´ä»½é™åˆ¶: {min(years)}-{max(years)}")
        
        search = arxiv.Search(
            query=final_query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡
        candidates = []
        filtered_by_year = 0
        filtered_by_keywords = 0
        
        try:
            for r in self.client.results(search):
                # å¹´ä»½è¿‡æ»¤
                paper_year = r.published.year
                if years and paper_year not in years:
                    filtered_by_year += 1
                    continue
                
                title = r.title.strip()
                abstract = r.summary.replace("\n", " ").strip()
                
                # å…³é”®è¯äºŒæ¬¡è¿‡æ»¤
                if filter_by_keywords and not matches_keywords(title, abstract, keywords):
                    filtered_by_keywords += 1
                    continue
                
                arxiv_id = get_arxiv_id_from_url(r.entry_id)
                
                candidates.append({
                    'title': title,
                    'abstract': abstract,
                    'authors': ", ".join([a.name for a in r.authors]),
                    'categories': ", ".join(r.categories),
                    'url': r.entry_id,
                    'year': str(paper_year),
                    'arxiv_id': arxiv_id
                })
                
        except Exception as e:
            print(f"âŒ Arxiv çˆ¬å–é”™è¯¯: {e}")
        
        # æ‰“å°åˆæ­¥ç»Ÿè®¡
        if years:
            print(f"   ğŸ“… å¹´ä»½è¿‡æ»¤: {filtered_by_year} ç¯‡è¢«è¿‡æ»¤")
        if filter_by_keywords:
            print(f"   ğŸ”‘ å…³é”®è¯è¿‡æ»¤: {filtered_by_keywords} ç¯‡è¢«è¿‡æ»¤")
        
        # ç¬¬äºŒéï¼šè·å–å¼•ç”¨é‡å¹¶è¿‡æ»¤
        results = []
        filtered_by_citations = 0
        
        if filter_by_citations and candidates:
            print(f"   ğŸ“Š è·å–å¼•ç”¨é‡ä¸­ï¼ˆå…± {len(candidates)} ç¯‡ï¼‰...")
            arxiv_ids = [c['arxiv_id'] for c in candidates if c['arxiv_id']]
            citation_counts = get_citation_count_batch(arxiv_ids)
            
            for c in candidates:
                citations = citation_counts.get(c['arxiv_id'], 0)
                
                if citations < self.min_citations:
                    filtered_by_citations += 1
                    continue
                
                paper = PaperData(
                    title=c['title'],
                    abstract=c['abstract'],
                    authors=c['authors'],
                    institutions="N/A",
                    venue=f"Arxiv ({citations} citations)",
                    url=c['url'],
                    year=c['year'],
                    keywords=c['categories']
                )
                results.append(paper)
            
            print(f"   ğŸ“ˆ å¼•ç”¨é‡è¿‡æ»¤ (>={self.min_citations}): {filtered_by_citations} ç¯‡è¢«è¿‡æ»¤")
        else:
            # ä¸åšå¼•ç”¨é‡è¿‡æ»¤
            for c in candidates:
                paper = PaperData(
                    title=c['title'],
                    abstract=c['abstract'],
                    authors=c['authors'],
                    institutions="N/A",
                    venue="Arxiv Preprint",
                    url=c['url'],
                    year=c['year'],
                    keywords=c['categories']
                )
                results.append(paper)
        
        print(f"   ğŸ“Š Arxiv: å…± {len(results)} ç¯‡è®ºæ–‡")
        
        return results
