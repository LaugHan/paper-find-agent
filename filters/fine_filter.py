"""
LLM ç­›é€‰å™¨ - æ”¯æŒå¹¶å‘æ¨ç†
"""
import asyncio
from typing import List
from tqdm import tqdm

from config import LLMConfig
from crawlers.base import PaperData
from .base import SiliconFlowClient, parse_xml_response


class FineFilter:
    """LLM ç­›é€‰å™¨ï¼ˆæ”¯æŒå¹¶å‘ï¼‰"""
    
    def __init__(self, config: LLMConfig, prompt_template: str, concurrency: int = 10):
        """
        Args:
            config: LLM é…ç½®
            prompt_template: ç­›é€‰ prompt æ¨¡æ¿ï¼ŒåŒ…å« {title} å’Œ {abstract} å ä½ç¬¦
            concurrency: å¹¶å‘æ•°
        """
        self.client = SiliconFlowClient(config)
        self.prompt_template = prompt_template
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å­¦æœ¯è®ºæ–‡ç­›é€‰åŠ©æ‰‹ã€‚"
        self.concurrency = concurrency
    
    def build_prompt(self, title: str, abstract: str) -> str:
        """æ„å»ºç­›é€‰ prompt"""
        return self.prompt_template.format(title=title, abstract=abstract)
    
    def filter_papers(
        self,
        papers: List[PaperData],
        sleep_seconds: float = 0.0  # å¹¶å‘æ¨¡å¼ä¸‹ä¸éœ€è¦ sleep
    ) -> List[PaperData]:
        """
        æ‰¹é‡ç­›é€‰è®ºæ–‡ï¼ˆå¹¶å‘ï¼‰
        
        Args:
            papers: å¾…ç­›é€‰è®ºæ–‡åˆ—è¡¨
            sleep_seconds: å·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°å…¼å®¹æ€§
        
        Returns:
            é€šè¿‡ç­›é€‰çš„è®ºæ–‡åˆ—è¡¨
        """
        print(f"\nğŸ” [LLMç­›é€‰] å¼€å§‹ç­›é€‰ {len(papers)} ç¯‡è®ºæ–‡ï¼ˆå¹¶å‘æ•°: {self.concurrency}ï¼‰...")
        
        # æ„å»ºæ‰€æœ‰ prompts
        prompts = [self.build_prompt(p.title, p.abstract) for p in papers]
        
        # å¹¶å‘è°ƒç”¨
        results = asyncio.run(self._filter_batch_async(prompts, papers))
        
        print(f"   âœ… ç­›é€‰å®Œæˆï¼Œä¿ç•™ {len(results)} ç¯‡è®ºæ–‡")
        return results
    
    async def _filter_batch_async(
        self, 
        prompts: List[str], 
        papers: List[PaperData]
    ) -> List[PaperData]:
        """å¼‚æ­¥æ‰¹é‡ç­›é€‰"""
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        pbar = tqdm(total=len(prompts), desc="ç­›é€‰è¿›åº¦")
        
        results = []
        batch_size = self.concurrency * 2  # æ¯æ‰¹å¤„ç†çš„æ•°é‡
        
        for i in range(0, len(prompts), batch_size):
            batch_prompts = prompts[i:i + batch_size]
            batch_papers = papers[i:i + batch_size]
            
            # å¹¶å‘è°ƒç”¨è¿™ä¸€æ‰¹
            responses = await self.client.call_batch_async(
                batch_prompts,
                self.system_prompt,
                concurrency=self.concurrency
            )
            
            # å¤„ç†å“åº”
            for paper, response in zip(batch_papers, responses):
                if response:
                    is_relevant, reason, abstract_zh = parse_xml_response(response)
                    if is_relevant:
                        paper.reason_zh = reason
                        paper.abstract_zh = abstract_zh
                        results.append(paper)
                pbar.update(1)
            
            # çŸ­æš‚ä¼‘æ¯é¿å…é™æµ
            await asyncio.sleep(0.5)
        
        pbar.close()
        return results


# é»˜è®¤çš„ç­›é€‰ prompt æ¨¡æ¿
DEFAULT_FINE_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç§‘ç ”åŠ©æ‰‹ã€‚æˆ‘ä¼šæä¾›ä¸€ç¯‡è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦ï¼Œè¯·ä½ åˆ¤æ–­è¿™ç¯‡è®ºæ–‡æ˜¯å¦ä¸ç ”ç©¶ä¸»é¢˜å¯†åˆ‡ç›¸å…³ã€‚

ç›¸å…³æ€§çš„åˆ¤å®šæ ‡å‡†æ˜¯ï¼šè®ºæ–‡éœ€è¦å®è´¨æ€§åœ°è®¨è®ºç ”ç©¶ä¸»é¢˜ï¼Œè€Œä¸ä»…ä»…æ˜¯é¡ºå¸¦æåˆ°è¿™äº›è¯ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢çš„æ ¼å¼ç”¨ä¸­æ–‡è¾“å‡ºï¼Œä¸è¦æ·»åŠ å¤šä½™è¯´æ˜ï¼š

<is_relevant>
true æˆ– false
</is_relevant>

<reason_zh>
å¦‚æœæ˜¯ trueï¼Œç”¨ä¸€ä¸¤å¥è¯ä¸­æ–‡è§£é‡Šä¸ºä»€ä¹ˆç›¸å…³ï¼›å¦‚æœæ˜¯ falseï¼Œè¿™ä¸€æ®µå¯ä»¥ç•™ç©ºã€‚
</reason_zh>

<abstract_zh>
å¦‚æœæ˜¯ trueï¼Œè¯·å°†è®ºæ–‡æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ï¼›å¦‚æœæ˜¯ falseï¼Œè¿™ä¸€æ®µå¯ä»¥ç•™ç©ºã€‚
</abstract_zh>

è®ºæ–‡æ ‡é¢˜: {title}

è®ºæ–‡æ‘˜è¦: {abstract}
"""
